{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b19c4f-e3de-48df-b60a-ce15e556de08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'metadata/lexicon_vmd.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lexicon \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      2\u001b[0m all_phones \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmetadata/lexicon_vmd.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m         word, \u001b[38;5;241m*\u001b[39mphonemes \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/noise/lib/python3.9/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'metadata/lexicon_vmd.txt'"
     ]
    }
   ],
   "source": [
    "lexicon = {}\n",
    "all_phones = []\n",
    "with open('metadata/lexicon_vmd.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, *phonemes = line.strip().split(' ')\n",
    "        lexicon[word] = phonemes\n",
    "        all_phones.extend(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0048a8c9-7805-40f8-b571-7a8eb0fea308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_phonemes = sorted(set(all_phones))\n",
    "len(list_phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff8a7c0-ca60-44db-a3de-cad41aee9df9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_phonemes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m list_phonemes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mlist_phonemes\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmdd\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;130;01m\\v\u001b[39;00m\u001b[38;5;124mi_phonemes.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(list_phonemes))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_phonemes' is not defined"
     ]
    }
   ],
   "source": [
    "list_phonemes = ['<pad>'] + list(list_phonemes)\n",
    "\n",
    "with open('mdd/db/vi_phonemes.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(' '.join(list_phonemes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7527a6",
   "metadata": {},
   "source": [
    "# Create HuggingFace Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e43e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f8880d8-2bed-4e38-b791-809c1d9d35c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('mdd/db/vi_phonemes.txt', 'r', encoding='utf-8') as f:\n",
    "    phonemes = f.read().split(' ')\n",
    "len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfaf2412-b5ca-4134-a5e1-d1552c41dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir tokenizer_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9612e2e0-7c7a-4fd2-9847-8acd7635a22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'a-0': 1, 'a-1': 2, 'a-2': 3, 'a-3': 4, 'a-4': 5, 'a-5': 6, 'aː-0': 7, 'aː-1': 8, 'aː-2': 9, 'aː-3': 10, 'aː-4': 11, 'aː-5': 12, 'e-0': 13, 'e-1': 14, 'e-2': 15, 'e-3': 16, 'e-4': 17, 'e-5': 18, 'eaː-0': 19, 'eaː-1': 20, 'eaː-2': 21, 'eaː-3': 22, 'eaː-4': 23, 'eaː-5': 24, 'f': 25, 'h': 26, 'i-0': 27, 'i-1': 28, 'i-2': 29, 'i-3': 30, 'i-4': 31, 'i-5': 32, 'iz': 33, 'iə-0': 34, 'iə-1': 35, 'iə-2': 36, 'iə-3': 37, 'iə-4': 38, 'iə-5': 39, 'k': 40, 'kpz': 41, 'kz': 42, 'k̟z': 43, 'l': 44, 'm': 45, 'mz': 46, 'n': 47, 'nz': 48, 'o-0': 49, 'o-1': 50, 'o-2': 51, 'o-3': 52, 'o-4': 53, 'o-5': 54, 'p': 55, 'pz': 56, 's': 57, 't': 58, 'tz': 59, 'tʰ': 60, 't͡ɕ': 61, 'u-0': 62, 'u-1': 63, 'u-2': 64, 'u-3': 65, 'u-4': 66, 'u-5': 67, 'uz': 68, 'uə-0': 69, 'uə-1': 70, 'uə-2': 71, 'uə-3': 72, 'uə-4': 73, 'uə-5': 74, 'v': 75, 'w': 76, 'x': 77, 'z': 78, 'ŋ': 79, 'ŋmz': 80, 'ŋz': 81, 'ŋ̟z': 82, 'ɓ': 83, 'ɔ-0': 84, 'ɔ-1': 85, 'ɔ-2': 86, 'ɔ-3': 87, 'ɔ-4': 88, 'ɔ-5': 89, 'ɗ': 90, 'ə-0': 91, 'ə-1': 92, 'ə-2': 93, 'ə-3': 94, 'ə-4': 95, 'ə-5': 96, 'əː-0': 97, 'əː-1': 98, 'əː-2': 99, 'əː-3': 100, 'əː-4': 101, 'əː-5': 102, 'ɛ-0': 103, 'ɛ-1': 104, 'ɛ-2': 105, 'ɛ-3': 106, 'ɛ-4': 107, 'ɛ-5': 108, 'ɣ': 109, 'ɨ-0': 110, 'ɨ-1': 111, 'ɨ-2': 112, 'ɨ-3': 113, 'ɨ-4': 114, 'ɨ-5': 115, 'ɨə-0': 116, 'ɨə-1': 117, 'ɨə-2': 118, 'ɨə-3': 119, 'ɨə-4': 120, 'ɨə-5': 121, 'ɲ': 122}\n"
     ]
    }
   ],
   "source": [
    "vocab = dict(zip(phonemes, range(len(phonemes))))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79d37358-8d77-4f85-b444-5d04bb429846",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['|'] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87342606-0037-4cf2-8d23-eb9aac5b8fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls tokenizer_hf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0239c49e-d0b7-4793-8e0b-dafd9a6c7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec_tokens = ['|', '<s>', '</s>', '<unk>']\n",
    "\n",
    "# for t in spec_tokens:\n",
    "#     vocab[t] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08fde46b-3ed0-4b1a-add5-d54463f5b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('tokenizer_hf/vocab.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91b47048-6b7a-4d9a-9a0d-88992d376694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'analyst & create data.ipynb'   tokenizer_hf\t  vocab.json\n",
      " mdd\t\t\t        tokenizer.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68580ef6-d50f-4e99-978c-4d39e9e70960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2CTCTokenizer(name_or_path='', vocab_size=124, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '<pad>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer('./tokenizer_hf/vocab.json', bos_token=None, eos_token=None, unk_token='<pad>', pad_token='<pad>')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1d08e475-e95b-4251-9b66-0f1c30a40d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tuanio/vi-phoneme-ctc-tokenizer/commit/09af8f669e680cb40ed160bbeafbbdee460adb41', commit_message='Upload tokenizer', commit_description='', oid='09af8f669e680cb40ed160bbeafbbdee460adb41', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('vi-phoneme-ctc-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6925f01e-2cb3-46fd-a396-18be103b5a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aː-0   m aː-4   ɗ aː-2   k ɔ-4   ɓ ɛ-4   l e-0   k ɔ-4   h aː-1   l aː-1   k w aː-1   k w i-4'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'aː-0 $ m aː-4 $ ɗ aː-2 $ k ɔ-4 $ ɓ ɛ-4 $ l e-0 $ k ɔ-4 $ h aː-1 $ l aː-1 $ k w aː-1 $ k w i-4'.replace('$', ' ')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "86feb549-7680-4eee-a6a2-64df2c4daed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aː-0', 'm', 'aː-4', 'ɗ', 'aː-2', 'k', 'ɔ-4', 'ɓ', 'ɛ-4', 'l', 'e-0', 'k', 'ɔ-4', 'h', 'aː-1', 'l', 'aː-1', 'k', 'w', 'aː-1', 'k', 'w', 'i-4']\n"
     ]
    }
   ],
   "source": [
    "tokens = [i for i in s.split(' ') if i]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c053a163-e74e-4e4c-a9fa-abbafd02550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = [' '.join(tokens), ' '.join(tokens + tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c452d2f4-571d-448e-a7ab-d1c94ae99397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aː-0 m aː-4 ɗ aː-2 k ɔ-4 ɓ ɛ-4 l e-0 k ɔ-4 h aː-1 l aː-1 k w aː-1 k w i-4',\n",
       " 'aː-0 m aː-4 ɗ aː-2 k ɔ-4 ɓ ɛ-4 l e-0 k ɔ-4 h aː-1 l aː-1 k w aː-1 k w i-4 aː-0 m aː-4 ɗ aː-2 k ɔ-4 ɓ ɛ-4 l e-0 k ɔ-4 h aː-1 l aː-1 k w aː-1 k w i-4']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b042d03e-6f8f-4dde-9e6c-adf7bce6d2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aː-0maː-4ɗaː-2kɔ-4ɓɛ-4le-0kɔ-4haː-1laː-1k waː-1k wi-4',\n",
       " 'aː-0maː-4ɗaː-2kɔ-4ɓɛ-4le-0kɔ-4haː-1laː-1k waː-1k wi-4aː-0maː-4ɗaː-2kɔ-4ɓɛ-4le-0kɔ-4haː-1laː-1k waː-1k wi-4']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokenizer(tok, padding=True, return_tensors='pt').input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2498c31a-a0b9-43f8-9822-1c6fa518f354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25, 50])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(tok, padding=True, return_tensors='pt').attention_mask.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "51a701a5-a17f-4a4d-b76e-9e19d1341fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aː-0 [7]\n",
      "m [45]\n",
      "aː-4 [11]\n",
      "ɗ [90]\n",
      "aː-2 [9]\n",
      "k [40]\n",
      "ɔ-4 [88]\n",
      "ɓ [83]\n",
      "ɛ-4 [107]\n",
      "l [44]\n",
      "e-0 [13]\n",
      "k [40]\n",
      "ɔ-4 [88]\n",
      "h [26]\n",
      "aː-1 [8]\n",
      "l [44]\n",
      "aː-1 [8]\n",
      "k [40]\n",
      "w [76]\n",
      "aː-1 [8]\n",
      "k [40]\n",
      "w [76]\n",
      "i-4 [31]\n"
     ]
    }
   ],
   "source": [
    "for i in tokens:\n",
    "    enc = tokenizer.encode(i)\n",
    "    print(i, enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1dbcb4e5-6e61-4a83-bf40-301dbbdcdee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aː-0',\n",
       " 'm',\n",
       " 'aː-4',\n",
       " 'ɗ',\n",
       " 'aː-2',\n",
       " 'k',\n",
       " 'ɔ-4',\n",
       " 'ɓ',\n",
       " 'ɛ-4',\n",
       " 'l',\n",
       " 'e-0',\n",
       " 'k',\n",
       " 'ɔ-4',\n",
       " 'h',\n",
       " 'aː-1',\n",
       " 'l',\n",
       " 'aː-1',\n",
       " 'k',\n",
       " '',\n",
       " 'w',\n",
       " 'aː-1',\n",
       " 'k',\n",
       " '',\n",
       " 'w',\n",
       " 'i-4']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode([i]) for i in tokenizer.encode(s)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
