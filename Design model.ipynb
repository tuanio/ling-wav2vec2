{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a84dc8be-7160-4d82-aeba-19b9a8caa6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torchaudio import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50e16e56-c7c9-4ad4-bdad-190127e5be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 80, 320)\n",
    "x_len = torch.LongTensor([320, 320, 320, 320])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a236146e-c424-40b3-bb9a-9e238ac925c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dec706e-32ff-45ab-ae7d-0af8ff4115fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 5 required positional arguments: 'input_dim', 'num_heads', 'ffn_dim', 'num_layers', and 'depthwise_conv_kernel_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 5 required positional arguments: 'input_dim', 'num_heads', 'ffn_dim', 'num_layers', and 'depthwise_conv_kernel_size'"
     ]
    }
   ],
   "source": [
    "models.Conformer(50, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5b7ad66-d608-427e-8cc9-d37196356772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_sub_block(\n",
    "    input_dim, model_dim, kernel_size=3, stride=2,\n",
    "):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(input_dim, model_dim, kernel_size=3, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "def calc_length(lengths, padding=1, kernel_size=3, stride=2, ceil_mode=False, repeat_num=1):\n",
    "    add_pad: float = (padding * 2) - kernel_size\n",
    "    one: float = 1.0\n",
    "    for i in range(repeat_num):\n",
    "        lengths = torch.div(lengths.to(dtype=torch.float) + add_pad, stride) + one\n",
    "        if ceil_mode:\n",
    "            lengths = torch.ceil(lengths)\n",
    "        else:\n",
    "            lengths = torch.floor(lengths)\n",
    "    return lengths.to(dtype=torch.int)\n",
    "\n",
    "class EffConformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=80,\n",
    "        model_dim=144,\n",
    "        num_heads=4,\n",
    "        num_layers=[2, 2, 4],\n",
    "        model_kernel_size=15,\n",
    "        vocab_size=123\n",
    "    ):\n",
    "        super().__init__()\n",
    "        down = []\n",
    "        model = []\n",
    "        self.num_down = len(num_layers)\n",
    "        for i in range(self.num_down):\n",
    "            down.append(init_sub_block(input_dim, model_dim))\n",
    "            model.append(models.Conformer(model_dim, num_heads, model_dim*4,\n",
    "                                          num_layers[i], model_kernel_size))\n",
    "            input_dim = model_dim\n",
    "        self.down = nn.ModuleList(down)\n",
    "        self.model = nn.ModuleList(model)\n",
    "        self.out = nn.Linear(model_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, x_len):\n",
    "        for i in range(len(self.down)):\n",
    "            x = self.down[i](x)\n",
    "            x_len = calc_length(x_len)\n",
    "            x = x.permute(0, 2, 1)\n",
    "            x, x_len = self.model[i](x, x_len)\n",
    "            if i < self.num_down - 1:\n",
    "                x = x.permute(0, 2, 1)\n",
    "        o = self.out(x)\n",
    "        return o, x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b0e08c81-47fe-4a1b-81d4-c68337dabd20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(EffConformer(\n",
       "   (down): ModuleList(\n",
       "     (0): Sequential(\n",
       "       (0): Conv1d(80, 144, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "       (1): ReLU()\n",
       "     )\n",
       "     (1-2): 2 x Sequential(\n",
       "       (0): Conv1d(144, 144, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "       (1): ReLU()\n",
       "     )\n",
       "   )\n",
       "   (model): ModuleList(\n",
       "     (0-1): 2 x Conformer(\n",
       "       (conformer_layers): ModuleList(\n",
       "         (0-1): 2 x ConformerLayer(\n",
       "           (ffn1): _FeedForwardModule(\n",
       "             (sequential): Sequential(\n",
       "               (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "               (1): Linear(in_features=144, out_features=576, bias=True)\n",
       "               (2): SiLU()\n",
       "               (3): Dropout(p=0.0, inplace=False)\n",
       "               (4): Linear(in_features=576, out_features=144, bias=True)\n",
       "               (5): Dropout(p=0.0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "           (self_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=144, out_features=144, bias=True)\n",
       "           )\n",
       "           (self_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv_module): _ConvolutionModule(\n",
       "             (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "             (sequential): Sequential(\n",
       "               (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
       "               (1): GLU(dim=1)\n",
       "               (2): Conv1d(144, 144, kernel_size=(15,), stride=(1,), padding=(7,), groups=144)\n",
       "               (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "               (4): SiLU()\n",
       "               (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
       "               (6): Dropout(p=0.0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (ffn2): _FeedForwardModule(\n",
       "             (sequential): Sequential(\n",
       "               (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "               (1): Linear(in_features=144, out_features=576, bias=True)\n",
       "               (2): SiLU()\n",
       "               (3): Dropout(p=0.0, inplace=False)\n",
       "               (4): Linear(in_features=576, out_features=144, bias=True)\n",
       "               (5): Dropout(p=0.0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (final_layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (2): Conformer(\n",
       "       (conformer_layers): ModuleList(\n",
       "         (0-3): 4 x ConformerLayer(\n",
       "           (ffn1): _FeedForwardModule(\n",
       "             (sequential): Sequential(\n",
       "               (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "               (1): Linear(in_features=144, out_features=576, bias=True)\n",
       "               (2): SiLU()\n",
       "               (3): Dropout(p=0.0, inplace=False)\n",
       "               (4): Linear(in_features=576, out_features=144, bias=True)\n",
       "               (5): Dropout(p=0.0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "           (self_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=144, out_features=144, bias=True)\n",
       "           )\n",
       "           (self_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "           (conv_module): _ConvolutionModule(\n",
       "             (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "             (sequential): Sequential(\n",
       "               (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
       "               (1): GLU(dim=1)\n",
       "               (2): Conv1d(144, 144, kernel_size=(15,), stride=(1,), padding=(7,), groups=144)\n",
       "               (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "               (4): SiLU()\n",
       "               (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
       "               (6): Dropout(p=0.0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (ffn2): _FeedForwardModule(\n",
       "             (sequential): Sequential(\n",
       "               (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "               (1): Linear(in_features=144, out_features=576, bias=True)\n",
       "               (2): SiLU()\n",
       "               (3): Dropout(p=0.0, inplace=False)\n",
       "               (4): Linear(in_features=576, out_features=144, bias=True)\n",
       "               (5): Dropout(p=0.0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (final_layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (out): Linear(in_features=144, out_features=123, bias=True)\n",
       " ),\n",
       " 4044507)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EffConformer()\n",
    "model, sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f78e0d48-27e7-4209-a671-38b4027d4d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 40, 123]), tensor([40, 40, 40, 40], dtype=torch.int32))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o, o_len = model(x, x_len)\n",
    "o.size(), o_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f2c030-89b9-4136-9128-6b82fb8e2542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
